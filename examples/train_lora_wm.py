#!/usr/bin/env python3
"""
LoRA Watermarking Training Script
==================================

ç”¨æ³•ï¼š
    python train_lora_watermark.py --config configs/lora_watermark_stylegan2.yaml
    
    æˆ–ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–ï¼š
    python train_lora_watermark.py \
        --config configs/lora_watermark_stylegan2. yaml \
        --lora_rank 16 \
        --batch_size 8 \
        --message "MYLOGO"
"""

import argparse
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„ï¼ˆæŠŠé¡¹ç›®æ ¹åŠ å…¥ï¼Œè€Œä¸æ˜¯ examples ç›®å½•ï¼‰
# è¿™æ · `import src.*` èƒ½åœ¨è¿è¡Œè¯¥ç¤ºä¾‹è„šæœ¬æ—¶è¢«æ­£ç¡®è§£æã€‚
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.models.lora_stylegan import LoRAStyleGAN
from src.watermarking_key. lora_wm_key import LoRAWatermarkingKey
from src.trainer.lora_trainer import LoRAWatermarkTrainer
from src.utils.config_loader import ConfigLoader


def parse_args():
    parser = argparse.ArgumentParser(description="Train LoRA Watermarking for StyleGAN")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # é…ç½®æ–‡ä»¶
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    parser.add_argument(
        '--config',
        type=str,
        required=True,
        help='Path to YAML config file'
    )
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # å¯é€‰è¦†ç›–å‚æ•°
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    parser.add_argument('--model_ckpt', type=str, help='Override model checkpoint path')
    parser.add_argument('--message', type=str, help='Override watermark message')
    parser. add_argument('--lora_rank', type=int, help='Override LoRA rank')
    parser.add_argument('--lora_alpha', type=float, help='Override LoRA alpha')
    parser.add_argument('--batch_size', type=int, help='Override batch size')
    parser.add_argument('--lr', type=float, help='Override learning rate')
    parser.add_argument('--device', type=str, choices=['cuda', 'cpu'], help='Override device')
    parser.add_argument('--output_ckpt', type=str, help='Override output checkpoint path')
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # å…¶ä»–é€‰é¡¹
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    parser.add_argument('--resume', action='store_true', help='Resume from checkpoint')
    parser.add_argument('--debug', action='store_true', help='Enable debug mode (disable wandb)')
    
    return parser.parse_args()


def main():
    args = parse_args()
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 1. åŠ è½½é…ç½®
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print(f"\nğŸ”§ Loading configuration from: {args.config}")
    model_args, wm_key_args, embed_args, env_args = ConfigLoader.load_from_yaml(args.config)
    
    # åº”ç”¨å‘½ä»¤è¡Œè¦†ç›–
    if args. model_ckpt:
        model_args.model_ckpt = args.model_ckpt
    if args.message:
        wm_key_args.message = args.message
        wm_key_args.bitlen = len(args.message) * 5
    if args.lora_rank:
        embed_args.lora_rank = args.lora_rank
    if args.lora_alpha:
        embed_args.lora_alpha = args.lora_alpha
    if args.batch_size:
        env_args.batch_size = args.batch_size
    if args.lr:
        embed_args.ptw_lr = args.lr
    if args. device:
        env_args. device = args.device
    if args.output_ckpt:
        embed_args.ckpt = args.output_ckpt
    if args.debug:
        env_args.logging_tool = "none"
    
    # æ‰“å°é…ç½®
    ConfigLoader.print_config(model_args, wm_key_args, embed_args, env_args)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 2. åŠ è½½æ¨¡å‹
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("ğŸ“¦ Loading StyleGAN model...")
    generator = LoRAStyleGAN(model_args, env_args)
    
    if args.resume and os.path.exists(embed_args.ckpt):
        print(f"â™»ï¸  Resuming from checkpoint: {embed_args.ckpt}")
        checkpoint = torch.load(embed_args.ckpt, map_location=env_args.device)
        # åŠ è½½ Generatorï¼ˆåŒ…å« LoRAï¼‰
        generator.G.load_state_dict(checkpoint['G_ema'])
    else:
        print(f"ğŸ†• Loading pretrained model: {model_args.model_ckpt}")
        generator.load_network(model_args.model_ckpt)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 3. åˆå§‹åŒ–æ°´å° Key
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("ğŸ”‘ Initializing watermarking key...")
    wm_key = LoRAWatermarkingKey(wm_key_args, env_args)
    
    if args.resume and os.path.exists(embed_args.ckpt):
        print(f"â™»ï¸  Loading decoder from checkpoint...")
        checkpoint = torch.load(embed_args.ckpt, map_location=env_args. device)
        if 'decoder_state_dict' in checkpoint:
            wm_key.decoder.load_state_dict(checkpoint['decoder_state_dict'])
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 4. å¼€å§‹è®­ç»ƒ
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("ğŸš€ Starting LoRA watermarking training.. .\n")
    trainer = LoRAWatermarkTrainer(embed_args, env_args)
    
    try:
        trainer.train(
            generator=generator,
            wm_key=wm_key,
            lora_rank=embed_args.lora_rank,
            lora_alpha=embed_args.lora_alpha,
            target_layers=embed_args.target_layers
        )
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Training interrupted by user.")
        print(f"ğŸ’¾ Saving checkpoint to: {embed_args. ckpt}")
        trainer.save(embed_args.ckpt)
        print("âœ… Checkpoint saved successfully.")


if __name__ == "__main__":
    import torch
    main()